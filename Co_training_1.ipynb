{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eggochi/Proyecto_Galaxias/blob/main/Co_training_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2hWYnE7G3CwI"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#!unzip /content/drive/MyDrive/Proyecto_Galaxias/smallGZ1.zip -d /content/Imagenes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.utils.data import Dataset, Subset , DataLoader, SubsetRandomSampler\n",
        "\n",
        "from torchvision.datasets.folder import default_loader, IMG_EXTENSIONS, has_file_allowed_extension\n",
        "from torchvision.ops import sigmoid_focal_loss\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn.functional as F\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "3C0Hudl_3T2C"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CONFIGURACIÓN\n",
        "# ======================\n",
        "#Datos\n",
        "data_dir = '/content/Imagenes/smallGZ1'  # Cambia esta ruta a donde tengas tus imagenes\n",
        "cvs_file = '/content/drive/MyDrive/Proyecto_Galaxias/Copia de smallCSV_class.csv'  # Ruta al archivo CSV con rutas y etiquetas\n",
        "df=pd.read_csv(cvs_file)\n",
        "num_classes= 6\n",
        "\n",
        "partition_size = 0.2  # Proporción para validación y test\n",
        "\n",
        "#Entrenamiento\n",
        "batch_size = 16     # Ajusta si te da \"out of memory\" (usa 8 o 12)\n",
        "max_epochs = 25\n",
        "learning_rate = 0.0005\n",
        "k_folds = 5\n",
        "ft_parameters=['features.5','features.6','features.7','features.8']\n",
        "\n",
        "#Semi Suprervisado\n",
        "thresholds = {\n",
        "    0: 0.92,  # clase dominante\n",
        "    3: 0.90,\n",
        "    2: 0.85,\n",
        "    1: 0.85,\n",
        "    5: 0.80,\n",
        "    4: 0.75   # clase más escasa\n",
        "}\n",
        "\n",
        "#scheduler\n",
        "scheduler_patience = 1  # Número de épocas sin mejora antes de reducir LR\n",
        "scheduler_factor = 0.5   # Factor de reducción del LR\n",
        "min_lr = 1e-6           # LR mínimo\n",
        "\n",
        "#Early Stopping\n",
        "patience = 5        # Número de épocas sin mejora antes de parar\n",
        "min_delta = 0.001   # Mejora mínima para considerar que hay progreso\n",
        "\n",
        "#dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5zYALRW3k9k",
        "outputId": "c066f016-5b23-4265-c866-09d87e30c2cf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zsr60JO14gNt"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Dividir_datos(data, test_size: float = 0.2, validation: bool = True, random_state: int = 42\n",
        "                 ):\n",
        "    \"\"\"\n",
        "    Lee CSV y divide en train / (val, test).\n",
        "    Si validation=True: primero separa train / temp (test_size),\n",
        "    luego divide temp en validation y test a partes iguales.\n",
        "    Devuelve (train_df, val_df, test_df).\n",
        "    \"\"\"\n",
        "    df = data\n",
        "    if df.shape[0] == 0:\n",
        "        raise ValueError(f\"CSV vacío: {ruta_csv}\")\n",
        "\n",
        "    y=df.iloc[:,-1].values\n",
        "\n",
        "    datos_entrenamiento, datos_prueba = train_test_split(df, test_size=test_size, random_state=random_state, shuffle=True,stratify=y)\n",
        "    if validation:\n",
        "        # dividir el 'prueba' en validación y prueba (mitades)\n",
        "        datos_validacion, datos_prueba = train_test_split(datos_prueba, test_size=0.5, random_state=random_state, shuffle=True)\n",
        "        print(f\"Datos de entrenamiento: {len(datos_entrenamiento)}, Datos de validacion {len(datos_validacion)}, Datos de prueba {len(datos_prueba)}\")\n",
        "        return datos_entrenamiento.reset_index(drop=True), datos_validacion.reset_index(drop=True), datos_prueba.reset_index(drop=True)\n",
        "\n",
        "    print(f\"Datos de entrenamiento: {len(datos_entrenamiento)}, Datos de prueba: {len(datos_prueba)}\")\n",
        "    return datos_entrenamiento.reset_index(drop=True), datos_prueba.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "FwH5Ymta4rAu"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CSV_Dataset(Dataset):\n",
        "    def __init__(self, root, dataframe,filename_col=None, label_col=None, transform=None,\n",
        "                 loader=default_loader, extensions=IMG_EXTENSIONS):\n",
        "        \"\"\"\n",
        "        Dataset a partir de un DataFrame de pandas.\n",
        "        - root: ruta base de las imágenes\n",
        "        - dataframe: objeto pd.DataFrame con columnas de ruta y etiqueta\n",
        "        - filename_col / label_col: nombres de columnas (si no se especifican, usa primera y última)\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.loader = loader\n",
        "        self.transform = transform\n",
        "\n",
        "        df = dataframe\n",
        "        if df.shape[1] < 2:\n",
        "            raise ValueError(\"El DataFrame debe tener al menos 2 columnas (filename y clase).\")\n",
        "\n",
        "        # Columnas a usar\n",
        "        filename_col = filename_col or df.columns[0]\n",
        "        label_col = label_col or df.columns[-1]\n",
        "\n",
        "        # Clases y mapeo\n",
        "        self.classes = df[label_col].value_counts().index.tolist()\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "        # Construir lista de muestras válidas\n",
        "        self.samples = []\n",
        "        for fname, label in zip(df[filename_col], df[label_col]):\n",
        "            if pd.isna(fname) or pd.isna(label):\n",
        "                continue\n",
        "            path = os.path.join(root, f\"{fname}.jpeg\")\n",
        "            if os.path.exists(path) and has_file_allowed_extension(path, extensions):\n",
        "                self.samples.append((path, self.class_to_idx[label]))\n",
        "\n",
        "        self.targets = [s[1] for s in self.samples]\n",
        "\n",
        "    def add_samples(self, paths, labels):\n",
        "        \"\"\"\n",
        "        Agrega nuevas muestras (por ejemplo, pseudolabels) al dataset.\n",
        "        - paths: lista de rutas de imágenes\n",
        "        - labels: lista de etiquetas numéricas\n",
        "        \"\"\"\n",
        "        if len(paths) != len(labels):\n",
        "            raise ValueError(\"El número de paths y labels debe coincidir.\")\n",
        "        for path, label in zip(paths, labels):\n",
        "            if os.path.exists(path):\n",
        "                self.samples.append((path, label))\n",
        "                self.targets.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, target = self.samples[idx]\n",
        "        image = self.loader(path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class SubsetView(Dataset):\n",
        "    def __init__(self, base_dataset, indices, labels=None):\n",
        "        self.base = base_dataset\n",
        "        self.indices = list(indices)\n",
        "        self.labels = list(labels) if labels is not None else [-1]*len(indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.indices[i]\n",
        "        image, _ = self.base[idx]\n",
        "        label = self.labels[i]\n",
        "        return image, label\n",
        "\n",
        "    def add_samples(self, new_indices, new_labels):\n",
        "        self.indices.extend(new_indices)\n",
        "        self.labels.extend(new_labels)\n",
        "\n",
        "    def remove_indices(self, remove_indices):\n",
        "        keep = [i for i, idx in enumerate(self.indices) if idx not in remove_indices]\n",
        "        self.indices = [self.indices[i] for i in keep]\n",
        "        self.labels = [self.labels[i] for i in keep]"
      ],
      "metadata": {
        "id": "RnbkfJzy4saK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c7ccd80",
        "outputId": "fe288655-dd50-4a19-d735-5a96473a9f56"
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn # Added import statement\n",
        "\n",
        "# ======================\n",
        "# MODELO: ConvNeXt-Tiny\n",
        "# ======================\n",
        "\n",
        "# Load a pre-trained ConvNeXt-Tiny model\n",
        "convnext_model = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Get the number of input features for the classifier\n",
        "num_ftrs = convnext_model.classifier[2].in_features\n",
        "\n",
        "# Replace the classifier with a new one\n",
        "convnext_model.classifier[2] = nn.Sequential(\n",
        "    nn.Dropout(0.5), # Added dropout layer\n",
        "    nn.Linear(num_ftrs, num_classes)\n",
        ")\n",
        "\n",
        "# Move the model to the device\n",
        "convnext_model = convnext_model.to(device)\n",
        "\n",
        "print(\"ConvNeXt-Tiny model with modified final layer:\")\n",
        "print(convnext_model.classifier)\n",
        "\n",
        "# Freeze all parameters in the network\n",
        "for param in convnext_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the parameters in the classifier and the last few blocks\n",
        "for name, param in convnext_model.named_parameters():\n",
        "    if 'classifier' in name or 'features.7' in name or 'features.6' in name: # Adjust layer names based on ConvNeXt architecture\n",
        "        param.requires_grad = True"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNeXt-Tiny model with modified final layer:\n",
            "Sequential(\n",
            "  (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
            "  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "  (2): Linear(in_features=768, out_features=6, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3396a0d3"
      },
      "source": [
        "# ======================\n",
        "# MODELO: EfficientNetNet-V2-S\n",
        "# ======================\n",
        "\n",
        "efficientnetV2_model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
        "\n",
        "# Reemplazar la capa final (classifier) and add dropout\n",
        "num_features = efficientnetV2_model.classifier[1].in_features\n",
        "efficientnetV2_model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.5), # Added dropout layer\n",
        "    nn.Linear(num_features, num_classes)\n",
        ")\n",
        "efficientnetV2_model = efficientnetV2_model.to(device)\n",
        "\n",
        "# Congelar capas base, descongelando las últimas capas y el clasificador\n",
        "# Note: Layer names may vary between EfficientNet versions.\n",
        "# You might need to inspect model.named_parameters() to fine-tune unfreezing.\n",
        "ft_parameters = ['features.5','features.6','features.7','features.8'] # These may need adjustment for V2-S\n",
        "\n",
        "for name, param in efficientnetV2_model.named_parameters():\n",
        "    if 'classifier' in name:\n",
        "        param.requires_grad = True\n",
        "    elif any(ft_param in name for ft_param in ft_parameters):\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "# ======================\n",
        "# MODELO: EfficientNetB0\n",
        "# ======================\n",
        "\n",
        "# efficientnetB0_model= models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
        "\n",
        "# # Reemplazar la capa final (classifier) and add dropout\n",
        "# num_features = efficientnetB0_model.classifier[1].in_features\n",
        "# efficientnetB0_model.classifier = nn.Sequential(\n",
        "#     nn.Dropout(0.5), # Added dropout layer\n",
        "#     nn.Linear(num_features, num_classes)\n",
        "# )\n",
        "# efficientnetB0_model = efficientnetB0_model.to(device)\n",
        "\n",
        "# # Congelar capas base, descongelando las últimas capas y el clasificador\n",
        "# # Note: Layer names may vary between EfficientNet versions.\n",
        "# # You might need to inspect model.named_parameters() to fine-tune unfreezing.\n",
        "# ft_parameters = ['features.5','features.6','features.7','features.8']\n",
        "\n",
        "# for name, param in efficientnetB0_model.named_parameters():\n",
        "#     if 'classifier' in name:\n",
        "#         param.requires_grad = True\n",
        "#     elif any(ft_param in name for ft_param in ft_parameters):\n",
        "#         param.requires_grad = True\n",
        "#     else:\n",
        "#         param.requires_grad = False"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# FUNCIÓN DE PÉRDIDA, OPTIMIZADOR Y SCHEDULER\n",
        "# ========================================\n",
        "\n",
        "def build_training_components(models, learning_rate, scheduler_factor,\n",
        "                              scheduler_patience, min_lr, device):\n",
        "    \"\"\"\n",
        "    Crea automáticamente criterion, optimizer y scheduler para cada modelo.\n",
        "    \"\"\"\n",
        "    optimizers, schedulers = {}, {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        optimizers[name] = optim.Adam(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=learning_rate\n",
        "        )\n",
        "\n",
        "        schedulers[name] = ReduceLROnPlateau(\n",
        "            optimizers[name],\n",
        "            mode='max',\n",
        "            factor=scheduler_factor,\n",
        "            patience=scheduler_patience,\n",
        "            min_lr=min_lr\n",
        "        )\n",
        "\n",
        "    return optimizers, schedulers\n",
        "\n",
        "#criterion cross entropy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v1p7m5N3BSlP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, criterion, dataloader, device, scaler):\n",
        "    model.train() # Keep dropout active for training\n",
        "    running_loss = 0.0\n",
        "    loop = tqdm(dataloader, desc=\"Entrenando\", leave=False)\n",
        "\n",
        "    for inputs, labels in loop:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type=device.type):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(models, dataloader, device, n_passes=10): # Added n_passes for MC Dropout\n",
        "    \"\"\"Evalúa varios modelos y calcula el F1-score de cada uno.\"\"\"\n",
        "    f1_scores = {}\n",
        "    all_labels = []\n",
        "    all_preds = {name: [] for name in models.keys()}\n",
        "\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Validación\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        for name, model in models.items():\n",
        "            model.train() # Keep dropout active for MC Dropout\n",
        "            batch_preds = []\n",
        "            for _ in range(n_passes): # Perform multiple passes\n",
        "                outputs = model(inputs)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                batch_preds.append(preds.cpu().numpy())\n",
        "\n",
        "            # Simple majority vote for prediction for validation\n",
        "            batch_preds = np.array(batch_preds)\n",
        "            final_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=batch_preds)\n",
        "            all_preds[name].extend(final_preds)\n",
        "\n",
        "\n",
        "    for name, preds in all_preds.items():\n",
        "        f1_scores[name] = f1_score(all_labels, preds, average='weighted')\n",
        "\n",
        "    return f1_scores"
      ],
      "metadata": {
        "id": "7dFeoQi0dDcM"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def pseudolabeling(models, dataloader, inv_frequencies, device, thresholds, n_passes=10): # Added n_passes\n",
        "    \"\"\"Genera pseudolabels en el conjunto no etiquetado.\"\"\"\n",
        "    all_indices, all_probs, all_pseudolabels = [], [], []\n",
        "\n",
        "    # Assuming the dataloader is built from a SubsetView of the unlabeled data\n",
        "    # We need the original indices from the base dataset\n",
        "    base_dataset_indices = dataloader.dataset.indices\n",
        "\n",
        "    for idx, (inputs, _) in enumerate(tqdm(dataloader, desc=\"Pseudolabeling\", leave=False)):\n",
        "        inputs = inputs.to(device)\n",
        "        probs_sum = 0\n",
        "\n",
        "        for name, model in models.items():\n",
        "            model.train() # Keep dropout active for MC Dropout\n",
        "            avg_probs = 0\n",
        "            for _ in range(n_passes): # Perform multiple passes\n",
        "                outputs = model(inputs)\n",
        "                avg_probs += torch.softmax(outputs, dim=1)\n",
        "            probs_sum += avg_probs / n_passes\n",
        "\n",
        "\n",
        "        probs_mean = probs_sum / len(models)\n",
        "        confs, preds = torch.max(probs_mean, dim=1)\n",
        "\n",
        "        # Get the corresponding base dataset indices for this batch\n",
        "        batch_base_indices = base_dataset_indices[idx * dataloader.batch_size : (idx + 1) * dataloader.batch_size]\n",
        "\n",
        "        all_probs.extend(confs.cpu().numpy())\n",
        "        all_pseudolabels.extend(preds.cpu().numpy())\n",
        "        all_indices.extend(batch_base_indices)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'index': all_indices,\n",
        "        'pseudolabel': all_pseudolabels,\n",
        "        'conf': all_probs\n",
        "    })\n",
        "\n",
        "    #estratificación y filtrado\n",
        "    selected_indices, selected_labels = [], []\n",
        "    for label, group in df.groupby(\"pseudolabel\"):\n",
        "        th = thresholds.get(label, 0.9)\n",
        "        group = group[group[\"conf\"] > th]\n",
        "        # Use inverse frequencies for sampling\n",
        "        n = int(len(group) * inv_frequencies[label])\n",
        "        n = max(10, n) # Ensure at least 10 samples if available\n",
        "        group = group.sort_values(\"conf\", ascending=False).head(n)\n",
        "        selected_indices.extend(group[\"index\"].tolist())\n",
        "        selected_labels.extend(group[\"pseudolabel\"].tolist())\n",
        "\n",
        "    return selected_indices, selected_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def coTraining(models, datasets, criterion, device, inv_frequencies, thresholds, batch_size,\n",
        "               max_epochs=10, patience=3):\n",
        "\n",
        "    scaler = torch.amp.GradScaler()\n",
        "    best_f1 = {name: 0.0 for name in models.keys()}\n",
        "    best_model_wts={name: None for name in models.keys()}\n",
        "    while True:\n",
        "        #re/iniciar optimizadores, schedulers y lista de labels agragadas\n",
        "        new_labels_added = {name: 0 for name in models.keys()}\n",
        "        optimizers, schedulers = build_training_components(models, learning_rate, scheduler_factor,\n",
        "                              scheduler_patience, min_lr, device)\n",
        "\n",
        "        for name, model in models.items():\n",
        "            other_name = [n for n in models.keys() if n != name][0]\n",
        "\n",
        "            print(f\"\\n=== Entrenando {name} ===\")\n",
        "            loaders = {\n",
        "                \"label\": DataLoader(datasets[name][\"label\"], batch_size=batch_size, shuffle=True),\n",
        "                \"val\": DataLoader(datasets[\"val\"], batch_size=batch_size, shuffle=False)\n",
        "            }\n",
        "\n",
        "            # --- Entrenamiento\n",
        "            epoch=0\n",
        "            counter = 0\n",
        "            while epoch <= max_epochs and counter < patience:\n",
        "                train_one_epoch(model, optimizers[name], criterion, loaders[\"label\"], device, scaler)\n",
        "                f1_scores = validate({name: model}, loaders[\"val\"], device, n_passes=1) # validate with MC Dropout\n",
        "                improved = False\n",
        "                # Step the scheduler based on the validation F1 score\n",
        "                schedulers[name].step(f1_scores[name])\n",
        "                if f1_scores[name] > best_f1[name]:\n",
        "                      best_f1[name] = f1_scores[name]\n",
        "                      improved = True\n",
        "                      best_model_wts[name] = deepcopy(models[name].state_dict()) # Use deepcopy\n",
        "\n",
        "\n",
        "                if improved:\n",
        "                    counter = 0\n",
        "                else:\n",
        "                    counter += 1\n",
        "                epoch += 1\n",
        "\n",
        "\n",
        "            print(\"\\nEntrenamiento finalizado.\")\n",
        "            print(f\"Mejor F1 {name}: {best_f1[name]:.4f}\")\n",
        "            # Load the best weights\n",
        "            models[name].load_state_dict(best_model_wts[name])\n",
        "\n",
        "\n",
        "\n",
        "            # --- Pseudolabeling del modelo A para el modelo B ---\n",
        "            print(f\"Generando pseudolabels de {name} → {other_name}\")\n",
        "            # Create a DataLoader for the *unlabeled* data of the current model\n",
        "            unlabeled_dataloader = DataLoader(datasets[name][\"unlabel\"], batch_size=batch_size, shuffle=False)\n",
        "            pseudo_indices, pseudo_labels = pseudolabeling(\n",
        "                {name: model},\n",
        "                unlabeled_dataloader,\n",
        "                inv_frequencies,device, thresholds # pseudolabeling with MC Dropout\n",
        "            )\n",
        "\n",
        "            if len(pseudo_indices) == 0:\n",
        "                print(f\"No se generaron pseudolabels para {other_name} en esta iteración.\")\n",
        "                continue\n",
        "\n",
        "            # Add pseudolabels to the *labeled* set of the other model\n",
        "            datasets[other_name][\"label\"].add_samples(pseudo_indices, pseudo_labels)\n",
        "            # Remove the pseudolabeled samples from the *unlabeled* set of the current model\n",
        "            datasets[name][\"unlabel\"].remove_indices(pseudo_indices)\n",
        "\n",
        "            new_labels_added[name] = len(pseudo_indices)\n",
        "            print(f\"{name} generó {len(pseudo_indices)} pseudolabels para {other_name}\")\n",
        "\n",
        "        # Detener si ningún modelo generó nuevos pseudolabels\n",
        "        if all(v == 0 for v in new_labels_added.values()):\n",
        "            print(\"\\n✅ No se generaron nuevos pseudolabels. Co-training finalizado.\")\n",
        "            break\n",
        "    return best_model_wts"
      ],
      "metadata": {
        "id": "fAUX0DyqgoiJ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# TRANSFORMACIONES\n",
        "# ======================\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                     [0.229, 0.224, 0.225])\n",
        "                     ])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                     [0.229, 0.224, 0.225])\n",
        "                     ])\n",
        "\n",
        "# ======================\n",
        "# CARGA DE DATOS\n",
        "# ======================\n",
        "\n",
        "training_data, validation_data, test_data = Dividir_datos(data=df, test_size=partition_size, validation=True)\n",
        "\n",
        "base_dataset    = CSV_Dataset(root=data_dir,dataframe=training_data ,filename_col='OBJID', label_col='CLASS', transform=train_transform)\n",
        "val_dataset     = CSV_Dataset(root=data_dir,dataframe=validation_data,filename_col='OBJID', label_col='CLASS', transform=val_transform)\n",
        "test_dataset    = CSV_Dataset(root=data_dir,dataframe=test_data, transform=val_transform)\n",
        "\n",
        "# Crear listas de índices y etiquetas desde base_dataset\n",
        "all_indices = np.arange(len(base_dataset))\n",
        "all_labels = [label for _, label in base_dataset.samples]\n",
        "\n",
        "# Tomamos un porcentaje (20%) como datos etiquetados\n",
        "labeled_indices, unlabeled_indices, labels, _ = train_test_split(\n",
        "    all_indices, all_labels,\n",
        "    test_size=0.8,              # 80% no etiquetados\n",
        "    stratify=all_labels,        # mantener proporción por clase\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOtAMGZ_4pkL",
        "outputId": "cbf5d104-1d02-42dd-8b60-5c14810efe97"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos de entrenamiento: 5331, Datos de validacion 666, Datos de prueba 667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the frequency of the classes\n",
        "class_frequency = df['CLASS'].value_counts()\n",
        "\n",
        "# Display the normalized frequency\n",
        "print(\"Class Frequency:\")\n",
        "print(class_frequency)\n",
        "\n",
        "\n",
        "# Calculate the inverse of the normalized frequencies\n",
        "inverse_frequency = 1 / class_frequency\n",
        "\n",
        "# Normalize\n",
        "normalized_inverse_frequency = inverse_frequency / inverse_frequency.sum()\n",
        "\n",
        "# Convert to a PyTorch tensor and sort by class index\n",
        "# Assuming class indices correspond to the index of the sorted normalized_class_frequency\n",
        "class_weights = torch.tensor(normalized_inverse_frequency.sort_index().values, dtype=torch.float32)\n",
        "\n",
        "print(\"Inverse Normalized Frequency (Class Weights):\")\n",
        "print(class_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCE0den652Y-",
        "outputId": "1651bd04-683e-4587-8965-780eb23d1a1e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Frequency:\n",
            "CLASS\n",
            "0    4645\n",
            "3     998\n",
            "2     436\n",
            "1     414\n",
            "5     115\n",
            "4      56\n",
            "Name: count, dtype: int64\n",
            "Inverse Normalized Frequency (Class Weights):\n",
            "tensor([0.0066, 0.0744, 0.0706, 0.0309, 0.5498, 0.2677])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# ========================================\n",
        "# USO\n",
        "# ========================================\n",
        "\n",
        "frequencies = {0: 4645, 3: 998, 2: 436, 1: 414, 5: 115, 4: 56}\n",
        "inv_freq = {cls: 1 / np.log1p(freq) for cls, freq in frequencies.items()}\n",
        "\n",
        "# normalizamos a fracciones\n",
        "total = sum(inv_freq.values())\n",
        "fractions = {cls: val / total for cls, val in inv_freq.items()}\n",
        "\n",
        "models = {\n",
        "    #\"ResNet50\": resnet50_model,\n",
        "    #\"EfficientNetB0\": efficientnetB0_model\n",
        "    \"ConvNeXt-Tiny\": convnext_model,\n",
        "    \"EfficientNetV2-S\": efficientnetV2_model\n",
        "}\n",
        "\n",
        "criterion=nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "datasets = {\n",
        "    \"val\": val_dataset,\n",
        "    \"test\": test_dataset\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    datasets[name] = {\n",
        "        \"label\": SubsetView(base_dataset, labeled_indices.copy(), labels.copy()),\n",
        "        \"unlabel\": SubsetView(base_dataset, unlabeled_indices.copy()) }\n",
        "\n",
        "models_wgts=coTraining(models, datasets, criterion,\n",
        "               device, fractions, thresholds, batch_size, max_epochs=20, patience=5)\n",
        "\n",
        "#========================================\n",
        "# TEST\n",
        "#========================================\n",
        "dataloaderTest=DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.load_state_dict(models_wgts[name])\n",
        "f1_scores = validate(models, dataloaderTest, device)\n",
        "print(\"\\nResultados en el conjunto de prueba:\")\n",
        "for name, score in f1_scores.items():\n",
        "    print(f\"{name:15s} | F1: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUVonhUNzzrb",
        "outputId": "dc904cf2-11e5-4c68-dce3-e5b6eb1b2a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Entrenando ConvNeXt-Tiny ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entrenamiento finalizado.\n",
            "Mejor F1 ConvNeXt-Tiny: 0.7801\n",
            "Generando pseudolabels de ConvNeXt-Tiny → EfficientNetV2-S\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNeXt-Tiny generó 355 pseudolabels para EfficientNetV2-S\n",
            "\n",
            "=== Entrenando EfficientNetV2-S ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entrenamiento finalizado.\n",
            "Mejor F1 EfficientNetV2-S: 0.7757\n",
            "Generando pseudolabels de EfficientNetV2-S → ConvNeXt-Tiny\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientNetV2-S generó 349 pseudolabels para ConvNeXt-Tiny\n",
            "\n",
            "=== Entrenando ConvNeXt-Tiny ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entrenamiento finalizado.\n",
            "Mejor F1 ConvNeXt-Tiny: 0.7801\n",
            "Generando pseudolabels de ConvNeXt-Tiny → EfficientNetV2-S\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNeXt-Tiny generó 307 pseudolabels para EfficientNetV2-S\n",
            "\n",
            "=== Entrenando EfficientNetV2-S ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entrenamiento finalizado.\n",
            "Mejor F1 EfficientNetV2-S: 0.7757\n",
            "Generando pseudolabels de EfficientNetV2-S → ConvNeXt-Tiny\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pseudolabeling:   0%|          | 1/245 [00:00<02:25,  1.67it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d911dd02"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}